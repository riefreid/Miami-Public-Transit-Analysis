{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2890e6-a88d-43c7-a051-86a998a214a5",
   "metadata": {},
   "source": [
    "Get Incident Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b95b93-2620-4db6-a391-aaefc99d455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "def fetch_traffic_incidents():\n",
    "    \"\"\"\n",
    "    Fetch traffic incidents from the TomTom API and save them to a CSV file.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.tomtom.com/traffic/services/5/incidentDetails\"\n",
    "    api_key = \"EWG2MPTB8cGkcoGTG7ujDZGxIMFGwNuT\"  # Replace with your TomTom API key\n",
    "    bbox = \"-80.391083,25.745477,-80.177879,25.837595\"\n",
    "    fields = \"{incidents{geometry{coordinates},properties{id,iconCategory,magnitudeOfDelay,events{description},startTime,from,to,length,delay,probabilityOfOccurrence}}}\"\n",
    "    url = f\"{base_url}?key={api_key}&bbox={bbox}&fields={fields}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "\n",
    "        data = response.json()\n",
    "        incidents = data.get('incidents', [])  # Extract incidents list from JSON data\n",
    "\n",
    "        csv_file = 'incidentdetails.csv'\n",
    "        file_exists = os.path.isfile(csv_file)  # Check if CSV file already exists\n",
    "\n",
    "        with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "            fieldnames = [\n",
    "                'geometry', 'magnitude_of_delay', 'description', 'start_time',\n",
    "                'from', 'to', 'length', 'prob_of_occur'\n",
    "            ]\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "\n",
    "            for incident in incidents:\n",
    "                geometry = incident.get('geometry', {})\n",
    "                coordinates = geometry.get('coordinates', [])\n",
    "\n",
    "                if coordinates:\n",
    "                    if len(coordinates) > 1:\n",
    "                        line = LineString(coordinates)\n",
    "                        geom = line.wkt  # Convert LineString to WKT format\n",
    "                    else:\n",
    "                        lon, lat = coordinates[0]\n",
    "                        geom = Point(lon, lat).wkt  # Convert Point to WKT format\n",
    "                else:\n",
    "                    geom = None\n",
    "\n",
    "                properties = incident.get('properties', {})\n",
    "                magnitude_of_delay = properties.get('magnitudeOfDelay', '')\n",
    "                events = properties.get('events', [{}])[0]  # Get first event (if available)\n",
    "                \n",
    "                description = events.get('description', '')\n",
    "                start_time = properties.get('startTime', '')\n",
    "                incident_from = properties.get('from', '')\n",
    "                incident_to = properties.get('to', '')\n",
    "                length = properties.get('length', '')\n",
    "                prob_of_occur = properties.get('probabilityOfOccurrence', '')\n",
    "\n",
    "                writer.writerow({\n",
    "                    'geometry': geom,\n",
    "                    'magnitude_of_delay': magnitude_of_delay,\n",
    "                    'description': description,\n",
    "                    'start_time': start_time,\n",
    "                    'from': incident_from,\n",
    "                    'to': incident_to,\n",
    "                    'length': length,\n",
    "                    'prob_of_occur': prob_of_occur\n",
    "                })\n",
    "\n",
    "        if incidents:\n",
    "            print(\"Traffic incident data was saved to incidentdetails.csv.\")\n",
    "        else:\n",
    "            print(\"No traffic incidents found or data could not be saved to CSV.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching traffic incidents: {e}\")\n",
    "\n",
    "# Example usage: Fetch traffic incidents and append to CSV file\n",
    "fetch_traffic_incidents()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f32fd-7310-4e5c-9c92-50cf7ef0d4e3",
   "metadata": {},
   "source": [
    "Get Traffic Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d3a57-7690-4f1e-9120-cb9a4b135ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from shapely.wkt import loads  # Import loads function for parsing WKT geometries\n",
    "\n",
    "def fetch_traffic_flow(latitude, longitude, api_key):\n",
    "    \"\"\"\n",
    "    Fetch traffic flow data from the TomTom API for a specific location.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.tomtom.com/traffic/services/4/flowSegmentData/relative0/13/json?key={api_key}&unit=KMPH&openLr=false\"\n",
    "    point_param = f\"{latitude},{longitude}\"  # Correct order: latitude, longitude\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params={'point': point_param})\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        \n",
    "        data = response.json()\n",
    "        flow_data = data.get('flowSegmentData', {})\n",
    "\n",
    "        return {\n",
    "            'current_speed': flow_data.get('currentSpeed', None),\n",
    "            'free_flow_speed': flow_data.get('freeFlowSpeed', None),\n",
    "            'current_travel_time': flow_data.get('currentTravelTime', None),\n",
    "            'free_flow_travel_time': flow_data.get('freeFlowTravelTime', None)\n",
    "        }\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching traffic flow data for point {latitude},{longitude}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_incidents_and_flow(input_csv, output_csv, api_key):\n",
    "    \"\"\"\n",
    "    Process traffic incidents and fetch corresponding traffic flow data.\n",
    "    Save the combined data to a new CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    current_speeds = []\n",
    "    free_flow_speeds = []\n",
    "    current_travel_times = []\n",
    "    free_flow_travel_times = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if isinstance(row['geometry'], str):\n",
    "            line = loads(row['geometry'])  # Parse WKT LineString\n",
    "            first_coord = list(line.coords)[0]  # Get the first coordinate pair (lon, lat)\n",
    "            longitude, latitude = first_coord\n",
    "\n",
    "            traffic_flow_data = fetch_traffic_flow(latitude, longitude, api_key)\n",
    "            \n",
    "            if traffic_flow_data:\n",
    "                current_speeds.append(traffic_flow_data['current_speed'])\n",
    "                free_flow_speeds.append(traffic_flow_data['free_flow_speed'])\n",
    "                current_travel_times.append(traffic_flow_data['current_travel_time'])\n",
    "                free_flow_travel_times.append(traffic_flow_data['free_flow_travel_time'])\n",
    "            else:\n",
    "                current_speeds.append(None)\n",
    "                free_flow_speeds.append(None)\n",
    "                current_travel_times.append(None)\n",
    "                free_flow_travel_times.append(None)\n",
    "        else:\n",
    "            current_speeds.append(None)\n",
    "            free_flow_speeds.append(None)\n",
    "            current_travel_times.append(None)\n",
    "            free_flow_travel_times.append(None)\n",
    "    \n",
    "    df['current_speed'] = current_speeds\n",
    "    df['free_flow_speed'] = free_flow_speeds\n",
    "    df['current_travel_time'] = current_travel_times\n",
    "    df['free_flow_travel_time'] = free_flow_travel_times\n",
    "    \n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Processed data saved to {output_csv}\")\n",
    "\n",
    "# Specify input and output CSV file paths and the API key\n",
    "input_csv_file = 'incidentdetails.csv'\n",
    "output_csv_file = 'incidentflow.csv'\n",
    "api_key = 'EWG2MPTB8cGkcoGTG7ujDZGxIMFGwNuT'  # Replace with your actual TomTom API key\n",
    "\n",
    "# Process incidents and traffic flow data\n",
    "process_incidents_and_flow(input_csv_file, output_csv_file, api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfee76-f7cb-4736-b6a4-38b255e3d23f",
   "metadata": {},
   "source": [
    "Feature engineering, pruning, and transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23680b93-53f9-45d8-80b0-1be31f199f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "from shapely.wkt import loads\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "# Load the input file\n",
    "file_path = 'incidentflow.csv'\n",
    "incidentflow_gdf = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows with NaN values and reset index\n",
    "incidentflow_gdf = incidentflow_gdf.dropna().reset_index(drop=True)\n",
    "\n",
    "# Parse geometry column\n",
    "def parse_linestring(geom_str):\n",
    "    try:\n",
    "        return loads(geom_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing geometry: {e}\")\n",
    "        return None\n",
    "\n",
    "incidentflow_gdf['geometry'] = incidentflow_gdf['geometry'].apply(parse_linestring)\n",
    "\n",
    "# Filter out rows with invalid geometries\n",
    "incidentflow_gdf = incidentflow_gdf[incidentflow_gdf['geometry'].notnull()]\n",
    "\n",
    "# Label encode probabilityOfOccurrence\n",
    "category_mapping = {'certain': 1, 'probable': 0}\n",
    "encoder = OrdinalEncoder(categories=[sorted(category_mapping, key=category_mapping.get)])\n",
    "incidentflow_gdf['risk'] = encoder.fit_transform(incidentflow_gdf[['prob_of_occur']])\n",
    "\n",
    "# One-hot encode description feature\n",
    "onehot_encoder = OneHotEncoder()\n",
    "desc_encoded = onehot_encoder.fit_transform(incidentflow_gdf[['description']])\n",
    "desc_encoded_df = pd.DataFrame(desc_encoded.toarray(), columns=onehot_encoder.categories_[0])\n",
    "incidentflow_gdf = pd.concat([incidentflow_gdf, desc_encoded_df], axis=1)\n",
    "\n",
    "# Rename encoded description feature columns\n",
    "desc_names = {\n",
    "    'Closed': 'is_closed_traffic',\n",
    "    'Heavy traffic': 'is_heavy_traffic',\n",
    "    'Queuing traffic': 'is_queuing_traffic',\n",
    "    'Roadworks': 'is_roadworks',\n",
    "    'Slow traffic': 'is_slow_traffic',\n",
    "    'Stationary traffic': 'is_stationary_traffic',\n",
    "    'Accident': 'is_accident',\n",
    "    'Bridge closed': 'is_bridge_closed',\n",
    "    'Incident': 'is_incident',\n",
    "    'Lane closed': 'is_lane_closed'\n",
    "}\n",
    "incidentflow_gdf.rename(columns=desc_names, inplace=True)\n",
    "incidentflow_gdf = incidentflow_gdf.loc[:, ~incidentflow_gdf.columns.duplicated()]\n",
    "\n",
    "# Function to add unique ID based on LineString geometry\n",
    "def add_unique_id(df):\n",
    "    df['id'] = ''\n",
    "    id_counter = 1\n",
    "    line_string_to_id = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        line_geom = row['geometry']\n",
    "        is_different = True\n",
    "\n",
    "        for existing_line_string in line_string_to_id.keys():\n",
    "            existing_line_geom = LineString(existing_line_string)\n",
    "            if line_geom.equals_exact(existing_line_geom, tolerance=1e-6):\n",
    "                df.at[index, 'id'] = line_string_to_id[existing_line_string]\n",
    "                is_different = False\n",
    "                break\n",
    "\n",
    "        if is_different:\n",
    "            df.at[index, 'id'] = id_counter\n",
    "            line_string_to_id[tuple(line_geom.coords)] = id_counter\n",
    "            id_counter += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "# Add unique IDs based on geometry\n",
    "incidentflow_gdf_with_id = add_unique_id(incidentflow_gdf)\n",
    "\n",
    "# Convert 'start_time' column to datetime\n",
    "incidentflow_gdf_with_id['start_time'] = pd.to_datetime(incidentflow_gdf_with_id['start_time'])\n",
    "incidentflow_gdf_with_id['day_of_week'] = incidentflow_gdf_with_id['start_time'].dt.dayofweek\n",
    "incidentflow_gdf_with_id['hour_of_day'] = incidentflow_gdf_with_id['start_time'].dt.hour\n",
    "\n",
    "# Move ID to first column\n",
    "id_column = incidentflow_gdf_with_id.pop('id')\n",
    "incidentflow_gdf_with_id.insert(0, 'id', id_column)\n",
    "\n",
    "# Define US federal holidays\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start=incidentflow_gdf_with_id['start_time'].min(), end=incidentflow_gdf_with_id['start_time'].max())\n",
    "\n",
    "# Check if each date is a holiday, weekday, or weekend\n",
    "incidentflow_gdf_with_id['is_holiday'] = incidentflow_gdf_with_id['start_time'].isin(holidays).astype(int)\n",
    "incidentflow_gdf_with_id['is_weekday'] = incidentflow_gdf_with_id['day_of_week'].isin(range(0, 5)).astype(int)\n",
    "incidentflow_gdf_with_id['is_weekend'] = incidentflow_gdf_with_id['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Convert these features to float type\n",
    "float_columns = ['current_speed', 'free_flow_speed', 'current_travel_time', 'free_flow_travel_time']\n",
    "incidentflow_gdf_with_id[float_columns] = incidentflow_gdf_with_id[float_columns].astype(float)\n",
    "\n",
    "# Convert these features to int type\n",
    "int_columns = [\n",
    "    'risk', 'is_bridge_closed', 'is_closed_traffic', 'is_lane_closed',\n",
    "    'is_queuing_traffic', 'is_roadworks', 'is_slow_traffic', 'is_stationary_traffic'\n",
    "]\n",
    "incidentflow_gdf_with_id[int_columns] = incidentflow_gdf_with_id[int_columns].astype(int)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "output_file_path = 'polished.csv'\n",
    "incidentflow_gdf_with_id.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display information about the modified DataFrame\n",
    "print(incidentflow_gdf_with_id.info())\n",
    "print(incidentflow_gdf_with_id['risk'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba97474-90f3-4e16-9aea-30cdfa73a9b6",
   "metadata": {},
   "source": [
    "After merging features in QGIS, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "200330ec-35e2-4e56-b298-86b4bc01ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 6666 entries, 0 to 6665\n",
      "Data columns (total 64 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   id                     6666 non-null   int64   \n",
      " 1   magnitude_of_delay     6666 non-null   int64   \n",
      " 2   description            6666 non-null   object  \n",
      " 3   start_time             6666 non-null   object  \n",
      " 4   from                   6666 non-null   object  \n",
      " 5   to                     6666 non-null   object  \n",
      " 6   length                 6666 non-null   float64 \n",
      " 7   prob_of_occur          6666 non-null   object  \n",
      " 8   current_speed          6666 non-null   float64 \n",
      " 9   free_flow_speed        6666 non-null   float64 \n",
      " 10  current_travel_time    6666 non-null   float64 \n",
      " 11  free_flow_travel_time  6666 non-null   float64 \n",
      " 12  risk                   6666 non-null   int64   \n",
      " 13  is_accident            6666 non-null   float64 \n",
      " 14  is_bridge_closed       6666 non-null   int64   \n",
      " 15  is_closed_traffic      6666 non-null   int64   \n",
      " 16  is_heavy_traffic       6666 non-null   float64 \n",
      " 17  is_incident            6666 non-null   float64 \n",
      " 18  is_lane_closed         6666 non-null   int64   \n",
      " 19  is_queuing_traffic     6666 non-null   int64   \n",
      " 20  is_roadworks           6666 non-null   int64   \n",
      " 21  is_slow_traffic        6666 non-null   int64   \n",
      " 22  is_stationary_traffic  6666 non-null   int64   \n",
      " 23  day_of_week            6666 non-null   int64   \n",
      " 24  hour_of_day            6666 non-null   int64   \n",
      " 25  is_holiday             6666 non-null   int64   \n",
      " 26  is_weekday             6666 non-null   int64   \n",
      " 27  is_weekend             6666 non-null   int64   \n",
      " 28  LINEABBR_count         4412 non-null   float64 \n",
      " 29  ZONE_DESC              6666 non-null   object  \n",
      " 30  n                      6666 non-null   int64   \n",
      " 31  distance               6666 non-null   float64 \n",
      " 32  feature_x              6666 non-null   float64 \n",
      " 33  feature_y              6666 non-null   float64 \n",
      " 34  nearest_x              6666 non-null   float64 \n",
      " 35  nearest_y              6666 non-null   float64 \n",
      " 36  AREALAND               6666 non-null   float64 \n",
      " 37  POP100                 6666 non-null   int64   \n",
      " 38  HU100                  6666 non-null   int64   \n",
      " 39  n_2                    6666 non-null   int64   \n",
      " 40  distance_2             6666 non-null   float64 \n",
      " 41  feature_x_2            6666 non-null   float64 \n",
      " 42  feature_y_2            6666 non-null   float64 \n",
      " 43  nearest_x_2            6666 non-null   float64 \n",
      " 44  nearest_y_2            6666 non-null   float64 \n",
      " 45  ONEWAY                 6471 non-null   object  \n",
      " 46  SPEEDLIMIT             6623 non-null   float64 \n",
      " 47  LANES                  6666 non-null   int64   \n",
      " 48  ST_WIDTH               6666 non-null   int64   \n",
      " 49  n_3                    6666 non-null   int64   \n",
      " 50  distance_3             6666 non-null   float64 \n",
      " 51  feature_x_3            6666 non-null   float64 \n",
      " 52  feature_y_3            6666 non-null   float64 \n",
      " 53  nearest_x_3            6666 non-null   float64 \n",
      " 54  nearest_y_3            6666 non-null   float64 \n",
      " 55  AADT                   6666 non-null   int64   \n",
      " 56  n_4                    6666 non-null   int64   \n",
      " 57  distance_4             6666 non-null   float64 \n",
      " 58  feature_x_4            6666 non-null   float64 \n",
      " 59  feature_y_4            6666 non-null   float64 \n",
      " 60  nearest_x_4            6666 non-null   float64 \n",
      " 61  nearest_y_4            6666 non-null   float64 \n",
      " 62  NUMPOINTS              6666 non-null   float64 \n",
      " 63  geometry               6666 non-null   geometry\n",
      "dtypes: float64(32), geometry(1), int64(24), object(7)\n",
      "memory usage: 3.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load the GeoDataFrame\n",
    "data = gpd.read_file('lights.geojson')\n",
    "\n",
    "# Display information about the GeoDataFrame\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f21efe5-15ec-4f74-8f5b-784726b8debd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shariefburns/opt/anaconda3/lib/python3.9/site-packages/geopandas/_compat.py:106: UserWarning: The Shapely GEOS version (3.11.3-CAPI-1.17.3) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 3213 entries, 0 to 6665\n",
      "Data columns (total 38 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   id                     3213 non-null   int64   \n",
      " 1   magnitude_of_delay     3213 non-null   int64   \n",
      " 2   from                   3213 non-null   object  \n",
      " 3   to                     3213 non-null   object  \n",
      " 4   length                 3213 non-null   float64 \n",
      " 5   flow_speed             3213 non-null   float64 \n",
      " 6   travel_time            3213 non-null   float64 \n",
      " 7   risk                   3213 non-null   int64   \n",
      " 8   is_heavy_traffic       3213 non-null   float64 \n",
      " 9   is_lane_closed         3213 non-null   int64   \n",
      " 10  is_queuing_traffic     3213 non-null   int64   \n",
      " 11  is_slow_traffic        3213 non-null   int64   \n",
      " 12  is_stationary_traffic  3213 non-null   int64   \n",
      " 13  day_of_week            3213 non-null   int64   \n",
      " 14  hour_of_day            3213 non-null   int64   \n",
      " 15  is_holiday             3213 non-null   int64   \n",
      " 16  is_weekday             3213 non-null   int64   \n",
      " 17  is_weekend             3213 non-null   int64   \n",
      " 18  num_routes             3213 non-null   float64 \n",
      " 19  zone_desc              3213 non-null   object  \n",
      " 20  oneway                 3213 non-null   object  \n",
      " 21  speed_limit            3213 non-null   float64 \n",
      " 22  num_lanes              3213 non-null   int64   \n",
      " 23  street_width           3213 non-null   int64   \n",
      " 24  aadt                   3213 non-null   int64   \n",
      " 25  num_lights             3213 non-null   float64 \n",
      " 26  geometry               3213 non-null   geometry\n",
      " 27  zone_group             3213 non-null   object  \n",
      " 28  commercial             3213 non-null   uint8   \n",
      " 29  government             3213 non-null   uint8   \n",
      " 30  industrial             3213 non-null   uint8   \n",
      " 31  interim                3213 non-null   uint8   \n",
      " 32  no_zone                3213 non-null   uint8   \n",
      " 33  planned                3213 non-null   uint8   \n",
      " 34  residential            3213 non-null   uint8   \n",
      " 35  urban                  3213 non-null   uint8   \n",
      " 36  pop_density            3213 non-null   float64 \n",
      " 37  bldg_density           3213 non-null   float64 \n",
      "dtypes: float64(9), geometry(1), int64(15), object(5), uint8(8)\n",
      "memory usage: 803.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Load the GeoDataFrame\n",
    "data = gpd.read_file('lights.geojson')\n",
    "\n",
    "# Rename columns\n",
    "rename_columns = {\n",
    "    'LINEABBR_count': 'num_routes',\n",
    "    'SPEEDLIMIT': 'speed_limit',\n",
    "    'LANES': 'num_lanes',\n",
    "    'ST_WIDTH': 'street_width',\n",
    "    'POP100': 'pop100',\n",
    "    'HU100': 'hu100',\n",
    "    'AREALAND': 'arealand',\n",
    "    'ZONE_DESC': 'zone_desc',\n",
    "    'current_speed': 'flow_speed',\n",
    "    'current_travel_time': 'travel_time',\n",
    "    'ONEWAY': 'oneway',\n",
    "    'AADT': 'aadt',\n",
    "}\n",
    "\n",
    "data = data.rename(columns=rename_columns)\n",
    "\n",
    "# Drop rows with NaN in 'speed_limit'\n",
    "data = data.dropna(subset=['speed_limit', 'oneway'])\n",
    "\n",
    "# Replace null values in 'num_routes' with zero\n",
    "data['num_routes'] = data['num_routes'].fillna(0)\n",
    "\n",
    "# Define the mapping dictionary\n",
    "zone_group_dict = {\n",
    "    'Bungalow Court District, 10,000 ft2 net': 'residential',\n",
    "    'Business Districts, liberal (wholesale) Includes mechanical garage and used car lots': 'commercial',\n",
    "    'Business Districts, limited': 'commercial',\n",
    "    'Business Districts, neighborhood': 'commercial',\n",
    "    'Business Districts, special': 'commercial',\n",
    "    'Corridor District': 'commercial',\n",
    "    'Four-unit Apartment District, 7,500 ft2 net': 'residential',\n",
    "    'Government Property': 'government',\n",
    "    'High Density Apartment House District, 50 units / net acre': 'residential',\n",
    "    'High Density Apartment House District, 50 units/net acre': 'residential',\n",
    "    'Industrial District, conditional': 'industrial',\n",
    "    'Industrial Districts, heavy manufacturing': 'industrial',\n",
    "    'Industrial Districts, light manufacturing': 'industrial',\n",
    "    'Industrial Districts, unlimited manufacturing': 'industrial',\n",
    "    'Interim District - Uses depend on character of neighborhood, otherwise EU-2 standards apply': 'interim',\n",
    "    'Limited Apartment House District, 23 units / net acre': 'residential',\n",
    "    'Minimun Apartment House 12.9 units/net acre': 'residential',\n",
    "    'Model City Urban Center District': 'urban',\n",
    "    'Modified Apartment House District, 35.9 units / net acre': 'residential',\n",
    "    'Modified Apartment House District, 35.9 units/net acre': 'residential',\n",
    "    'NO ZONING DESIGNATED': 'no_zone',\n",
    "    'Office Park District, 3 acres grossÿ Office buildings & laboratories for scientific and industrial research.': 'commercial',\n",
    "    'Palmer Lake Metropolitan Urban Center': 'urban',\n",
    "    'Planned Area Development, 20 acres minimum.ÿ Mixed residential and convenience retail services.ÿ Density depends on Master Plan, neighborhood studies and neighborhood development.': 'planned',\n",
    "    'RU-4 or Hotel/Motel District, 75 units / net acre': 'residential',\n",
    "    'RU-4 or Hotel/Motel District, 75 units/net acre': 'residential',\n",
    "    'Semi-professional Office District, 10,000 ft2 net': 'commercial',\n",
    "    'Single-family Residential District 7,500 ft2ÿnet': 'residential',\n",
    "    'Townhouse District,ÿ 8.5 units/net acre': 'residential',\n",
    "    'Two-family Residential District, 7,500 ft2 net': 'residential'\n",
    "}\n",
    "\n",
    "# Apply the mapping to create a new column 'zone_group'\n",
    "data['zone_group'] = data['zone_desc'].map(zone_group_dict).fillna('other')\n",
    "\n",
    "# One-hot encode 'zone_group'\n",
    "zone_group_encoded = pd.get_dummies(data['zone_group'])\n",
    "data = pd.concat([data, zone_group_encoded], axis=1)\n",
    "\n",
    "# Calculate new features\n",
    "if 'pop100' in data.columns and 'arealand' in data.columns:\n",
    "    data['pop_density'] = data['pop100'] / data['arealand']\n",
    "else:\n",
    "    print(\"Columns 'pop100' or 'arealand' not found, skipping 'pop_density' calculation\")\n",
    "\n",
    "if 'hu100' in data.columns and 'arealand' in data.columns:\n",
    "    data['bldg_density'] = data['hu100'] / data['arealand']\n",
    "else:\n",
    "    print(\"Columns 'hu100' or 'arealand' not found, skipping 'bldg_density' calculation\")\n",
    "\n",
    "\n",
    "# Drop columns only if they exist\n",
    "columns_to_drop = ['arealand', 'pop100', 'hu100']\n",
    "\n",
    "data = data.drop(columns=[col for col in columns_to_drop if col in data.columns])\n",
    "\n",
    "# Drop rows that don't meet the condition\n",
    "required_columns = ['n', 'n_2', 'n_3', 'n_4']\n",
    "if all(col in data.columns for col in required_columns):\n",
    "    data = data[(data['n'] == 1) & (data['n_2'] == 1) & (data['n_3'] == 1) & (data['n_4'] == 1)]\n",
    "else:\n",
    "    print(\"Required columns for filtering rows are not present in the DataFrame\")\n",
    "\n",
    "# List of columns to drop\n",
    "additional_columns_to_drop = [\n",
    "    'n', 'distance', 'feature_x', 'feature_y', 'nearest_x', 'nearest_y', \n",
    "    'n_2', 'distance_2', 'feature_x_2', 'feature_y_2', 'nearest_x_2', 'nearest_y_2', \n",
    "    'n_3', 'distance_3', 'feature_x_3', 'feature_y_3', 'nearest_x_3', 'nearest_y_3', \n",
    "    'n_4', 'distance_4', 'feature_x_4', 'feature_y_4', 'nearest_x_4', 'nearest_y_4',\n",
    "    'free_flow_speed', 'free_flow_travel_time', 'is_accident', 'is_bridge_closed', \n",
    "    'is_closed_traffic', 'is_incident', 'is_roadworks', 'prob_of_occur', 'description',\n",
    "    'start_time']\n",
    "\n",
    "# Drop the specified columns from the GeoDataFrame\n",
    "data = data.drop(columns=[col for col in additional_columns_to_drop if col in data.columns])\n",
    "\n",
    "# Print the updated DataFrame info\n",
    "print(data.info())\n",
    "data.to_csv('data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521d1d4-7068-4f50-9f7e-5b5567f3e746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
